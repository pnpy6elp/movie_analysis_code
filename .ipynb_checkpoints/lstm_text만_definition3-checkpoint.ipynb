{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce3e63c",
   "metadata": {},
   "source": [
    "# definition 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42af27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import codecs\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import nltk\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# train set\n",
    "name = \"3\"\n",
    "filename1 = './new_data/definition'+name+'_spynorth_scaling_trust.txt'\n",
    "filename2 = './new_data/definition'+name+'_spynorth_scaling_untrust.txt'\n",
    "filename3 = './new_data/definition'+name+'_intistranger_scaling_trust.txt'\n",
    "filename4 = './new_data/definition'+name+'_intistranger_scaling_untrust.txt'\n",
    "filename5 = './new_data/definition'+name+'_assassin_scaling_trust.txt'\n",
    "filename6 = './new_data/definition'+name+'_assassin_scaling_untrust.txt'\n",
    "filename7 = './new_data/definition'+name+'_1987_scaling_trust.txt'\n",
    "filename8 = './new_data/definition'+name+'_1987_scaling_untrust.txt'\n",
    "filename9 = './new_data/definition'+name+'_taxi_scaling_trust.txt'\n",
    "filename10 = './new_data/definition'+name+'_taxi_scaling_untrust.txt'\n",
    "\n",
    "\n",
    "with codecs.open(filename1, 'r', encoding='utf-8-sig') as f:\n",
    "    lines1 = f.readlines()\n",
    "with codecs.open(filename2, 'r', encoding='utf-8-sig') as f:\n",
    "    lines2 = f.readlines()\n",
    "with codecs.open(filename3, 'r', encoding='utf-8-sig') as f:\n",
    "    lines3 = f.readlines()\n",
    "with codecs.open(filename4, 'r', encoding='utf-8-sig') as f:\n",
    "    lines4 = f.readlines()\n",
    "with codecs.open(filename5, 'r', encoding='utf-8-sig') as f:\n",
    "    lines5 = f.readlines()\n",
    "with codecs.open(filename6, 'r', encoding='utf-8-sig') as f:\n",
    "    lines6 = f.readlines()\n",
    "with codecs.open(filename7, 'r', encoding='utf-8-sig') as f:\n",
    "    lines7 = f.readlines()\n",
    "with codecs.open(filename8, 'r', encoding='utf-8-sig') as f:\n",
    "    lines8 = f.readlines()\n",
    "with codecs.open(filename9, 'r', encoding='utf-8-sig') as f:\n",
    "    lines9 = f.readlines()\n",
    "with codecs.open(filename10, 'r', encoding='utf-8-sig') as f:\n",
    "    lines10 = f.readlines()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test set\n",
    "\n",
    "\n",
    "with codecs.open('./new_data/definition'+name+'_spynorth_test_t.txt', 'r', 'utf-8-sig') as f:\n",
    "    test1 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_spynorth_test_ut.txt', 'r', 'utf-8-sig') as f:\n",
    "    test2 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_intistranger_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test3 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_intistranger_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test4 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_assassin_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test5 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_assassin_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test6 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_1987_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test7 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_1987_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test8 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_taxi_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test9 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_taxi_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test10 = f.readlines()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17aa8b4",
   "metadata": {},
   "source": [
    "## 공작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21da864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1\n",
    "lines_ut = lines2\n",
    "test_t = test1\n",
    "test_ut = test2\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(0)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(1)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(0)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(1)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "input_test = np.asarray(feature2)\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "input_test = input_test.reshape(-1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edf4a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5120/5120 [00:28<00:00, 179.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:04<00:00, 304.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 25s 674ms/step - loss: 0.6931 - acc: 0.5002 - val_loss: 0.6924 - val_acc: 0.5449\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 24s 766ms/step - loss: 0.6866 - acc: 0.5906 - val_loss: 0.6895 - val_acc: 0.5264\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 27s 828ms/step - loss: 0.6816 - acc: 0.6565 - val_loss: 0.6878 - val_acc: 0.5459\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 24s 760ms/step - loss: 0.6422 - acc: 0.7107 - val_loss: 0.6903 - val_acc: 0.5635\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 26s 806ms/step - loss: 0.5747 - acc: 0.7278 - val_loss: 0.7233 - val_acc: 0.5566\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 26s 811ms/step - loss: 0.5249 - acc: 0.7617 - val_loss: 0.7649 - val_acc: 0.5420\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 26s 821ms/step - loss: 0.4703 - acc: 0.7861 - val_loss: 0.8014 - val_acc: 0.5439\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 26s 808ms/step - loss: 0.8281 - acc: 0.6731 - val_loss: 0.8691 - val_acc: 0.5371\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 25s 797ms/step - loss: 0.4722 - acc: 0.7583 - val_loss: 0.8558 - val_acc: 0.5332\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 26s 795ms/step - loss: 0.4313 - acc: 0.7937 - val_loss: 0.8386 - val_acc: 0.5391\n",
      "40/40 [==============================] - 4s 76ms/step - loss: 0.9233 - acc: 0.4906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9233145713806152, 0.4906249940395355]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1[:2560]\n",
    "lines_ut = lines2[:2560]\n",
    "test_t = test1[:640]\n",
    "test_ut = test2[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236be201",
   "metadata": {},
   "source": [
    "## 공작 + 완벽한 타인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7e26655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10240/10240 [00:44<00:00, 227.98it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2560/2560 [00:09<00:00, 283.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 46s 682ms/step - loss: 0.6928 - acc: 0.5115 - val_loss: 0.6914 - val_acc: 0.5391\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 50s 786ms/step - loss: 0.6767 - acc: 0.6021 - val_loss: 0.6881 - val_acc: 0.5547\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 53s 830ms/step - loss: 0.6635 - acc: 0.6260 - val_loss: 0.6999 - val_acc: 0.5464\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 49s 769ms/step - loss: 0.5978 - acc: 0.6901 - val_loss: 0.7431 - val_acc: 0.5371\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 53s 822ms/step - loss: 1.0343 - acc: 0.5905 - val_loss: 0.7687 - val_acc: 0.5283\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 54s 851ms/step - loss: 0.5591 - acc: 0.7134 - val_loss: 0.7556 - val_acc: 0.5283\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 50s 790ms/step - loss: 0.6182 - acc: 0.6899 - val_loss: 1.2567 - val_acc: 0.5024\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 51s 804ms/step - loss: 0.5653 - acc: 0.7257 - val_loss: 0.7874 - val_acc: 0.5288\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 51s 800ms/step - loss: 0.5783 - acc: 0.7272 - val_loss: 0.8157 - val_acc: 0.5317\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 52s 808ms/step - loss: 0.4510 - acc: 0.7876 - val_loss: 0.8612 - val_acc: 0.5205\n",
      "80/80 [==============================] - 7s 70ms/step - loss: 0.8974 - acc: 0.5039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8973738551139832, 0.50390625]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1[:2560] + lines3[:2560]\n",
    "lines_ut = lines2[:2560] + lines4[:2560]\n",
    "test_t = test1[:640] + test3[:640]\n",
    "test_ut = test2[:640] + test4[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ededfff",
   "metadata": {},
   "source": [
    "## 공작 + 완벽한 타인 + 암살"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c589a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 15360/15360 [01:04<00:00, 237.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3840/3840 [00:13<00:00, 277.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "96/96 [==============================] - 64s 649ms/step - loss: 0.6924 - acc: 0.5161 - val_loss: 0.6917 - val_acc: 0.5046\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 72s 754ms/step - loss: 0.6754 - acc: 0.5858 - val_loss: 0.6924 - val_acc: 0.5469\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 76s 791ms/step - loss: 0.6367 - acc: 0.6421 - val_loss: 0.7213 - val_acc: 0.5410\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 73s 758ms/step - loss: 0.8975 - acc: 0.6195 - val_loss: 0.7301 - val_acc: 0.5273\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 76s 792ms/step - loss: 0.5908 - acc: 0.6807 - val_loss: 0.7695 - val_acc: 0.5238\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 75s 788ms/step - loss: 0.5607 - acc: 0.7090 - val_loss: 0.7874 - val_acc: 0.5176\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 77s 806ms/step - loss: 0.5278 - acc: 0.7257 - val_loss: 0.8148 - val_acc: 0.5202\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 76s 794ms/step - loss: 0.5065 - acc: 0.7447 - val_loss: 0.8900 - val_acc: 0.5199\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 77s 803ms/step - loss: 0.4863 - acc: 0.7564 - val_loss: 0.8977 - val_acc: 0.5130\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 79s 827ms/step - loss: 2.4513 - acc: 0.5635 - val_loss: 2.2322 - val_acc: 0.5010\n",
      "120/120 [==============================] - 9s 69ms/step - loss: 2.2654 - acc: 0.5003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2654318809509277, 0.5002604126930237]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1[:2560] + lines3[:2560] + lines5[:2560]\n",
    "lines_ut = lines2[:2560] + lines4[:2560] + lines6[:2560]\n",
    "test_t = test1[:640] + test3[:640] + test5[:640]\n",
    "test_ut = test2[:640] + test4[:640] + test6[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce03ba",
   "metadata": {},
   "source": [
    "## 공작 + 완벽한 타인 + 암살 + 1987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a33c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 20480/20480 [01:37<00:00, 211.02it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5120/5120 [00:20<00:00, 252.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "128/128 [==============================] - 80s 611ms/step - loss: 0.6925 - acc: 0.5139 - val_loss: 0.6900 - val_acc: 0.5332\n",
      "Epoch 2/10\n",
      "128/128 [==============================] - 93s 727ms/step - loss: 0.6774 - acc: 0.5831 - val_loss: 0.6855 - val_acc: 0.5488\n",
      "Epoch 3/10\n",
      "128/128 [==============================] - 106s 826ms/step - loss: 0.6458 - acc: 0.6268 - val_loss: 0.7015 - val_acc: 0.5496\n",
      "Epoch 4/10\n",
      "128/128 [==============================] - 108s 843ms/step - loss: 0.6141 - acc: 0.6570 - val_loss: 0.7317 - val_acc: 0.5403\n",
      "Epoch 5/10\n",
      "128/128 [==============================] - 105s 821ms/step - loss: 0.5838 - acc: 0.6815 - val_loss: 0.7583 - val_acc: 0.5344\n",
      "Epoch 6/10\n",
      "128/128 [==============================] - 106s 827ms/step - loss: 0.5540 - acc: 0.7049 - val_loss: 0.8015 - val_acc: 0.5293\n",
      "Epoch 7/10\n",
      "128/128 [==============================] - 108s 842ms/step - loss: 0.5283 - acc: 0.7224 - val_loss: 0.8597 - val_acc: 0.5315\n",
      "Epoch 8/10\n",
      "128/128 [==============================] - 107s 837ms/step - loss: 0.5037 - acc: 0.7382 - val_loss: 0.9088 - val_acc: 0.5281\n",
      "Epoch 9/10\n",
      "128/128 [==============================] - 105s 819ms/step - loss: 0.4813 - acc: 0.7518 - val_loss: 0.9099 - val_acc: 0.5249\n",
      "Epoch 10/10\n",
      "128/128 [==============================] - 101s 792ms/step - loss: 0.4615 - acc: 0.7636 - val_loss: 0.9571 - val_acc: 0.5300\n",
      "160/160 [==============================] - 13s 75ms/step - loss: 1.0008 - acc: 0.4861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.000839114189148, 0.48613280057907104]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1[:2560] + lines3[:2560] + lines5[:2560] + lines7[:2560]\n",
    "lines_ut = lines2[:2560] + lines4[:2560] + lines6[:2560] + lines8[:2560]\n",
    "test_t = test1[:640] + test3[:640] + test5[:640] + test7[:640]\n",
    "test_ut = test2[:640] + test4[:640] + test6[:640] + test8[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17fb0fd",
   "metadata": {},
   "source": [
    "## 공작 + 완벽한 타인 + 암살 + 1987 + 택시운전사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c706e086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 25600/25600 [02:03<00:00, 207.24it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 6400/6400 [00:30<00:00, 212.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "160/160 [==============================] - 106s 650ms/step - loss: 0.6925 - acc: 0.5128 - val_loss: 0.6911 - val_acc: 0.5275\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 107s 666ms/step - loss: 0.6782 - acc: 0.5739 - val_loss: 0.6922 - val_acc: 0.5393\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 127s 794ms/step - loss: 0.6481 - acc: 0.6220 - val_loss: 0.7094 - val_acc: 0.5299\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 128s 799ms/step - loss: 0.6162 - acc: 0.6565 - val_loss: 0.7439 - val_acc: 0.5303\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 128s 801ms/step - loss: 0.5882 - acc: 0.6794 - val_loss: 0.7648 - val_acc: 0.5246\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 131s 817ms/step - loss: 0.5600 - acc: 0.7012 - val_loss: 0.7741 - val_acc: 0.5248\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 130s 813ms/step - loss: 0.5368 - acc: 0.7180 - val_loss: 0.8180 - val_acc: 0.5209\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 129s 803ms/step - loss: 0.5149 - acc: 0.7316 - val_loss: 0.8528 - val_acc: 0.5137\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 129s 809ms/step - loss: 0.4960 - acc: 0.7407 - val_loss: 0.9077 - val_acc: 0.5158\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 127s 795ms/step - loss: 0.4745 - acc: 0.7580 - val_loss: 0.9233 - val_acc: 0.5107\n",
      "200/200 [==============================] - 16s 75ms/step - loss: 0.9350 - acc: 0.5097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9350454807281494, 0.5096874833106995]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1[:2560] + lines3[:2560] + lines5[:2560] + lines7[:2560] + lines9[:2560]\n",
    "lines_ut = lines2[:2560] + lines4[:2560] + lines6[:2560] + lines8[:2560] + lines10[:2560]\n",
    "test_t = test1[:640] + test3[:640] + test5[:640] + test7[:640] + test9[:640]\n",
    "test_ut = test2[:640] + test4[:640] + test6[:640] + test8[:640] + test10[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c17ee6",
   "metadata": {},
   "source": [
    "## 완벽한 타인 + 암살"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c83a665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10240/10240 [00:58<00:00, 175.28it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2560/2560 [00:21<00:00, 118.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 50s 714ms/step - loss: 0.6931 - acc: 0.4990 - val_loss: 0.6925 - val_acc: 0.5293\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 50s 781ms/step - loss: 0.6840 - acc: 0.5771 - val_loss: 0.6912 - val_acc: 0.5186\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 43s 664ms/step - loss: 0.6490 - acc: 0.6311 - val_loss: 0.7082 - val_acc: 0.5337\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 44s 691ms/step - loss: 0.7191 - acc: 0.6338 - val_loss: 0.7707 - val_acc: 0.5215\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 44s 682ms/step - loss: 0.6122 - acc: 0.6637 - val_loss: 0.7124 - val_acc: 0.5220\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 52s 823ms/step - loss: 0.5903 - acc: 0.6954 - val_loss: 0.7469 - val_acc: 0.5317\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 55s 862ms/step - loss: 0.5543 - acc: 0.7122 - val_loss: 0.7827 - val_acc: 0.5249\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 57s 882ms/step - loss: 0.5064 - acc: 0.7493 - val_loss: 0.8122 - val_acc: 0.5166\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 52s 807ms/step - loss: 0.4995 - acc: 0.7494 - val_loss: 0.8326 - val_acc: 0.5117\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 46s 712ms/step - loss: 0.4562 - acc: 0.7766 - val_loss: 0.8775 - val_acc: 0.5156\n",
      "80/80 [==============================] - 7s 75ms/step - loss: 0.9102 - acc: 0.5051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9101732969284058, 0.505078136920929]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines3[:2560] + lines5[:2560] \n",
    "lines_ut = lines4[:2560] + lines6[:2560]\n",
    "test_t = test3[:640] + test5[:640] \n",
    "test_ut = test4[:640] + test6[:640] \n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef26b5",
   "metadata": {},
   "source": [
    "## 완벽한 타인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ad9f956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5120/5120 [00:37<00:00, 137.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:11<00:00, 115.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 28s 750ms/step - loss: 0.6931 - acc: 0.5054 - val_loss: 0.6926 - val_acc: 0.5186\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 26s 806ms/step - loss: 0.6864 - acc: 0.5928 - val_loss: 0.6914 - val_acc: 0.5127\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 26s 815ms/step - loss: 0.6584 - acc: 0.6489 - val_loss: 0.6984 - val_acc: 0.5283\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 26s 818ms/step - loss: 0.6426 - acc: 0.6777 - val_loss: 0.7091 - val_acc: 0.4980\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 25s 795ms/step - loss: 0.5829 - acc: 0.7305 - val_loss: 0.7256 - val_acc: 0.5020\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 26s 820ms/step - loss: 0.8161 - acc: 0.6064 - val_loss: 0.7558 - val_acc: 0.5166\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 23s 717ms/step - loss: 0.5411 - acc: 0.7427 - val_loss: 0.7523 - val_acc: 0.5078\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 18s 563ms/step - loss: 0.5038 - acc: 0.7739 - val_loss: 0.8429 - val_acc: 0.5049\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 16s 516ms/step - loss: 0.5914 - acc: 0.7368 - val_loss: 1.2347 - val_acc: 0.5068\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 0.9044 - acc: 0.6042 - val_loss: 0.7830 - val_acc: 0.5107\n",
      "40/40 [==============================] - 4s 70ms/step - loss: 0.8069 - acc: 0.4977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8068866729736328, 0.4976562559604645]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras.models import Model\n",
    "lines_t = lines3[:2560]\n",
    "lines_ut = lines4[:2560]\n",
    "test_t = test3[:640]\n",
    "test_ut = test4[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effabdd3",
   "metadata": {},
   "source": [
    "## 암살"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90bc0ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5120/5120 [00:39<00:00, 128.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:11<00:00, 106.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 29s 768ms/step - loss: 0.6932 - acc: 0.5078 - val_loss: 0.6931 - val_acc: 0.4980\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 24s 762ms/step - loss: 0.6873 - acc: 0.5920 - val_loss: 0.6933 - val_acc: 0.5107\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 25s 789ms/step - loss: 0.6605 - acc: 0.6404 - val_loss: 0.7242 - val_acc: 0.5068\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 24s 760ms/step - loss: 0.6148 - acc: 0.6750 - val_loss: 0.7260 - val_acc: 0.5107\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 25s 800ms/step - loss: 0.5729 - acc: 0.7190 - val_loss: 0.7370 - val_acc: 0.5078\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 26s 812ms/step - loss: 0.5282 - acc: 0.7539 - val_loss: 0.7778 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 23s 705ms/step - loss: 0.4857 - acc: 0.7715 - val_loss: 0.8263 - val_acc: 0.5244\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 19s 595ms/step - loss: 0.4581 - acc: 0.7849 - val_loss: 0.8331 - val_acc: 0.4971\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 17s 522ms/step - loss: 0.4465 - acc: 0.7932 - val_loss: 0.8594 - val_acc: 0.4863\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 17s 519ms/step - loss: 0.3956 - acc: 0.8218 - val_loss: 0.9276 - val_acc: 0.4961\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.9626 - acc: 0.5125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9625601768493652, 0.512499988079071]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines5[:2560]\n",
    "lines_ut = lines6[:2560]\n",
    "test_t = test5[:640]\n",
    "test_ut = test6[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91538ac",
   "metadata": {},
   "source": [
    "## 1987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d46026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5120/5120 [00:42<00:00, 121.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:11<00:00, 109.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 26s 682ms/step - loss: 0.6930 - acc: 0.5044 - val_loss: 0.6920 - val_acc: 0.5381\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 24s 752ms/step - loss: 0.6879 - acc: 0.5710 - val_loss: 0.6893 - val_acc: 0.5225\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 26s 798ms/step - loss: 0.6728 - acc: 0.5964 - val_loss: 0.6876 - val_acc: 0.5527\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 26s 809ms/step - loss: 0.6383 - acc: 0.6541 - val_loss: 0.6969 - val_acc: 0.5518\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 25s 783ms/step - loss: 0.5838 - acc: 0.7056 - val_loss: 0.7355 - val_acc: 0.5352\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 26s 819ms/step - loss: 0.7770 - acc: 0.6975 - val_loss: 0.7928 - val_acc: 0.5107\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 24s 756ms/step - loss: 0.5785 - acc: 0.7085 - val_loss: 0.7422 - val_acc: 0.5117\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 19s 591ms/step - loss: 0.5082 - acc: 0.7668 - val_loss: 0.7705 - val_acc: 0.5059\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 17s 516ms/step - loss: 0.4584 - acc: 0.7927 - val_loss: 0.8439 - val_acc: 0.4980\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 16s 512ms/step - loss: 0.4271 - acc: 0.8047 - val_loss: 0.9072 - val_acc: 0.4980\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.9252 - acc: 0.5086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9251649975776672, 0.508593738079071]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines7[:2560]\n",
    "lines_ut = lines8[:2560]\n",
    "test_t = test7[:640]\n",
    "test_ut = test8[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd14bb",
   "metadata": {},
   "source": [
    "## 택시운전사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d61d0f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5120/5120 [00:38<00:00, 133.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:11<00:00, 110.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 26s 682ms/step - loss: 0.6933 - acc: 0.5081 - val_loss: 0.6934 - val_acc: 0.4980\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 26s 800ms/step - loss: 0.6885 - acc: 0.5725 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 26s 797ms/step - loss: 0.6640 - acc: 0.6304 - val_loss: 0.7089 - val_acc: 0.5029\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 25s 764ms/step - loss: 1.1242 - acc: 0.5972 - val_loss: 0.7194 - val_acc: 0.5039\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 26s 800ms/step - loss: 0.5910 - acc: 0.7236 - val_loss: 0.7254 - val_acc: 0.5078\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 26s 804ms/step - loss: 0.5590 - acc: 0.7466 - val_loss: 0.7417 - val_acc: 0.5088\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 26s 823ms/step - loss: 0.5358 - acc: 0.7651 - val_loss: 0.7580 - val_acc: 0.5049\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 20s 625ms/step - loss: 0.5144 - acc: 0.7568 - val_loss: 0.8004 - val_acc: 0.4961\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 16s 501ms/step - loss: 0.4639 - acc: 0.7803 - val_loss: 0.8143 - val_acc: 0.5029\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 16s 494ms/step - loss: 0.4281 - acc: 0.8044 - val_loss: 0.8416 - val_acc: 0.5068\n",
      "40/40 [==============================] - 3s 60ms/step - loss: 0.8642 - acc: 0.5211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8642040491104126, 0.5210937261581421]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines9[:2560]\n",
    "lines_ut = lines10[:2560]\n",
    "test_t = test9[:640]\n",
    "test_ut = test10[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "383.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
