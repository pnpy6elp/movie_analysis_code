{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce3e63c",
   "metadata": {},
   "source": [
    "# definition 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42af27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import codecs\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import nltk\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# train set\n",
    "name = \"1\"\n",
    "filename1 = './new_data/definition'+name+'_spynorth_scaling_trust.txt'\n",
    "filename2 = './new_data/definition'+name+'_spynorth_scaling_untrust.txt'\n",
    "filename3 = './new_data/definition'+name+'_intistranger_scaling_trust.txt'\n",
    "filename4 = './new_data/definition'+name+'_intistranger_scaling_untrust.txt'\n",
    "filename5 = './new_data/definition'+name+'_assassin_scaling_trust.txt'\n",
    "filename6 = './new_data/definition'+name+'_assassin_scaling_untrust.txt'\n",
    "filename7 = './new_data/definition'+name+'_1987_scaling_trust.txt'\n",
    "filename8 = './new_data/definition'+name+'_1987_scaling_untrust.txt'\n",
    "filename9 = './new_data/definition'+name+'_taxi_scaling_trust.txt'\n",
    "filename10 = './new_data/definition'+name+'_taxi_scaling_untrust.txt'\n",
    "\n",
    "\n",
    "with codecs.open(filename1, 'r', encoding='utf-8-sig') as f:\n",
    "    lines1 = f.readlines()\n",
    "with codecs.open(filename2, 'r', encoding='utf-8-sig') as f:\n",
    "    lines2 = f.readlines()\n",
    "with codecs.open(filename3, 'r', encoding='utf-8-sig') as f:\n",
    "    lines3 = f.readlines()\n",
    "with codecs.open(filename4, 'r', encoding='utf-8-sig') as f:\n",
    "    lines4 = f.readlines()\n",
    "with codecs.open(filename5, 'r', encoding='utf-8-sig') as f:\n",
    "    lines5 = f.readlines()\n",
    "with codecs.open(filename6, 'r', encoding='utf-8-sig') as f:\n",
    "    lines6 = f.readlines()\n",
    "with codecs.open(filename7, 'r', encoding='utf-8-sig') as f:\n",
    "    lines7 = f.readlines()\n",
    "with codecs.open(filename8, 'r', encoding='utf-8-sig') as f:\n",
    "    lines8 = f.readlines()\n",
    "with codecs.open(filename9, 'r', encoding='utf-8-sig') as f:\n",
    "    lines9 = f.readlines()\n",
    "with codecs.open(filename10, 'r', encoding='utf-8-sig') as f:\n",
    "    lines10 = f.readlines()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test set\n",
    "\n",
    "\n",
    "with codecs.open('./new_data/definition'+name+'_spynorth_test_t.txt', 'r', 'utf-8-sig') as f:\n",
    "    test1 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_spynorth_test_ut.txt', 'r', 'utf-8-sig') as f:\n",
    "    test2 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_intistranger_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test3 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_intistranger_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test4 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_assassin_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test5 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_assassin_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test6 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_1987_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test7 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_1987_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test8 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_taxi_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test9 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_taxi_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test10 = f.readlines()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17aa8b4",
   "metadata": {},
   "source": [
    "## 공작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21da864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1\n",
    "lines_ut = lines2\n",
    "test_t = test1\n",
    "test_ut = test2\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(0)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(1)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(0)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(1)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "input_test = np.asarray(feature2)\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "input_test = input_test.reshape(-1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edf4a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5120/5120 [00:30<00:00, 167.94it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:10<00:00, 117.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 37s 951ms/step - loss: 0.6810 - acc: 0.5850 - val_loss: 0.6636 - val_acc: 0.6318\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 28s 882ms/step - loss: 0.6209 - acc: 0.6831 - val_loss: 0.6263 - val_acc: 0.6553\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 26s 808ms/step - loss: 0.5582 - acc: 0.7278 - val_loss: 0.6347 - val_acc: 0.6475\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 26s 810ms/step - loss: 0.5061 - acc: 0.7673 - val_loss: 0.6479 - val_acc: 0.6465\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 27s 843ms/step - loss: 0.4612 - acc: 0.7961 - val_loss: 0.6782 - val_acc: 0.6475\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 25s 788ms/step - loss: 0.4158 - acc: 0.8225 - val_loss: 0.7156 - val_acc: 0.6523\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 27s 824ms/step - loss: 0.3785 - acc: 0.8450 - val_loss: 0.7488 - val_acc: 0.6416\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 26s 805ms/step - loss: 0.3418 - acc: 0.8591 - val_loss: 0.8022 - val_acc: 0.6289\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 20s 610ms/step - loss: 0.3367 - acc: 0.8606 - val_loss: 0.8109 - val_acc: 0.6426\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 15s 470ms/step - loss: 0.2974 - acc: 0.8860 - val_loss: 0.8762 - val_acc: 0.6396\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 1.0839 - acc: 0.5258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0839489698410034, 0.5257812738418579]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1[:2560]\n",
    "lines_ut = lines2[:2560]\n",
    "test_t = test1[:640]\n",
    "test_ut = test2[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236be201",
   "metadata": {},
   "source": [
    "## 공작 + 완벽한 타인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7e26655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10240/10240 [00:49<00:00, 208.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2560/2560 [00:25<00:00, 98.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 61s 859ms/step - loss: 0.6832 - acc: 0.5603 - val_loss: 0.6641 - val_acc: 0.6006\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 52s 812ms/step - loss: 0.6293 - acc: 0.6584 - val_loss: 0.6474 - val_acc: 0.6270\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 52s 809ms/step - loss: 0.5809 - acc: 0.7026 - val_loss: 0.6596 - val_acc: 0.6338\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 52s 808ms/step - loss: 0.7870 - acc: 0.6719 - val_loss: 1.4808 - val_acc: 0.5220\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 50s 772ms/step - loss: 0.8551 - acc: 0.6283 - val_loss: 0.6961 - val_acc: 0.6133\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 52s 815ms/step - loss: 0.5066 - acc: 0.7614 - val_loss: 0.6996 - val_acc: 0.6133\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 51s 803ms/step - loss: 0.6224 - acc: 0.7274 - val_loss: 0.8062 - val_acc: 0.5474\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 54s 839ms/step - loss: 0.9693 - acc: 0.6151 - val_loss: 1.1067 - val_acc: 0.5347\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 51s 796ms/step - loss: 0.7978 - acc: 0.6257 - val_loss: 0.7273 - val_acc: 0.6045\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 29s 459ms/step - loss: 0.7332 - acc: 0.6436 - val_loss: 0.7831 - val_acc: 0.5752\n",
      "80/80 [==============================] - 5s 62ms/step - loss: 0.9158 - acc: 0.5172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9157848358154297, 0.5171874761581421]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1[:2560] + lines3[:2560]\n",
    "lines_ut = lines2[:2560] + lines4[:2560]\n",
    "test_t = test1[:640] + test3[:640]\n",
    "test_ut = test2[:640] + test4[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ededfff",
   "metadata": {},
   "source": [
    "## 공작 + 완벽한 타인 + 암살"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c589a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 15360/15360 [01:17<00:00, 198.25it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3840/3840 [00:39<00:00, 96.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "96/96 [==============================] - 83s 807ms/step - loss: 0.6781 - acc: 0.5710 - val_loss: 0.6576 - val_acc: 0.6266\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 76s 797ms/step - loss: 0.6264 - acc: 0.6557 - val_loss: 0.6475 - val_acc: 0.6289\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 80s 834ms/step - loss: 0.5922 - acc: 0.6898 - val_loss: 0.6559 - val_acc: 0.6240\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 76s 787ms/step - loss: 0.5601 - acc: 0.7149 - val_loss: 0.6706 - val_acc: 0.6182\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 78s 807ms/step - loss: 0.5350 - acc: 0.7351 - val_loss: 0.7252 - val_acc: 0.6003\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 76s 789ms/step - loss: 0.5148 - acc: 0.7515 - val_loss: 0.7242 - val_acc: 0.6029\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 76s 788ms/step - loss: 0.4953 - acc: 0.7625 - val_loss: 0.7561 - val_acc: 0.5960\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 76s 792ms/step - loss: 0.4804 - acc: 0.7706 - val_loss: 0.7798 - val_acc: 0.5990\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 70s 733ms/step - loss: 0.7519 - acc: 0.7000 - val_loss: 0.7588 - val_acc: 0.5882\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 45s 467ms/step - loss: 0.4765 - acc: 0.7709 - val_loss: 0.7774 - val_acc: 0.5814\n",
      "120/120 [==============================] - 8s 62ms/step - loss: 0.9268 - acc: 0.5115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9267807602882385, 0.5114583373069763]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1[:2560] + lines3[:2560] + lines5[:2560]\n",
    "lines_ut = lines2[:2560] + lines4[:2560] + lines6[:2560]\n",
    "test_t = test1[:640] + test3[:640] + test5[:640]\n",
    "test_ut = test2[:640] + test4[:640] + test6[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce03ba",
   "metadata": {},
   "source": [
    "## 공작 + 완벽한 타인 + 암살 + 1987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a33c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 20480/20480 [01:56<00:00, 176.42it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5120/5120 [00:57<00:00, 88.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "128/128 [==============================] - 110s 820ms/step - loss: 0.6743 - acc: 0.5835 - val_loss: 0.6547 - val_acc: 0.6116\n",
      "Epoch 2/10\n",
      "128/128 [==============================] - 107s 837ms/step - loss: 0.6254 - acc: 0.6573 - val_loss: 0.6553 - val_acc: 0.6094\n",
      "Epoch 3/10\n",
      "128/128 [==============================] - 104s 807ms/step - loss: 0.5940 - acc: 0.6893 - val_loss: 0.6687 - val_acc: 0.6123\n",
      "Epoch 4/10\n",
      "128/128 [==============================] - 106s 826ms/step - loss: 0.5678 - acc: 0.7071 - val_loss: 0.6810 - val_acc: 0.6174\n",
      "Epoch 5/10\n",
      "128/128 [==============================] - 106s 828ms/step - loss: 0.5425 - acc: 0.7268 - val_loss: 0.7106 - val_acc: 0.6006\n",
      "Epoch 6/10\n",
      "128/128 [==============================] - 107s 833ms/step - loss: 0.5201 - acc: 0.7422 - val_loss: 0.7207 - val_acc: 0.6011\n",
      "Epoch 7/10\n",
      "128/128 [==============================] - 103s 806ms/step - loss: 0.4999 - acc: 0.7552 - val_loss: 0.7634 - val_acc: 0.6001\n",
      "Epoch 8/10\n",
      "128/128 [==============================] - 103s 803ms/step - loss: 0.4956 - acc: 0.7654 - val_loss: 0.7555 - val_acc: 0.5950\n",
      "Epoch 9/10\n",
      "128/128 [==============================] - 83s 648ms/step - loss: 0.4661 - acc: 0.7746 - val_loss: 0.7922 - val_acc: 0.5894\n",
      "Epoch 10/10\n",
      "128/128 [==============================] - 58s 449ms/step - loss: 0.4509 - acc: 0.7843 - val_loss: 0.8223 - val_acc: 0.5894\n",
      "160/160 [==============================] - 10s 61ms/step - loss: 0.9341 - acc: 0.5270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9340513348579407, 0.5269531011581421]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1[:2560] + lines3[:2560] + lines5[:2560] + lines7[:2560]\n",
    "lines_ut = lines2[:2560] + lines4[:2560] + lines6[:2560] + lines8[:2560]\n",
    "test_t = test1[:640] + test3[:640] + test5[:640] + test7[:640]\n",
    "test_ut = test2[:640] + test4[:640] + test6[:640] + test8[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17fb0fd",
   "metadata": {},
   "source": [
    "## 공작 + 완벽한 타인 + 암살 + 1987 + 택시운전사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c706e086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 25600/25600 [02:47<00:00, 152.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6400/6400 [01:15<00:00, 84.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "160/160 [==============================] - 138s 826ms/step - loss: 0.6713 - acc: 0.5797 - val_loss: 0.6560 - val_acc: 0.6156\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 130s 815ms/step - loss: 0.6268 - acc: 0.6507 - val_loss: 0.6544 - val_acc: 0.6148\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 129s 805ms/step - loss: 0.6003 - acc: 0.6750 - val_loss: 0.6602 - val_acc: 0.6189\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 127s 795ms/step - loss: 0.5790 - acc: 0.6936 - val_loss: 0.6755 - val_acc: 0.6211\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 130s 811ms/step - loss: 0.5596 - acc: 0.7104 - val_loss: 0.6836 - val_acc: 0.6131\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 128s 798ms/step - loss: 0.5435 - acc: 0.7216 - val_loss: 0.6982 - val_acc: 0.6088\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 129s 810ms/step - loss: 0.5270 - acc: 0.7326 - val_loss: 0.7045 - val_acc: 0.6066\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 128s 798ms/step - loss: 0.5099 - acc: 0.7447 - val_loss: 0.7404 - val_acc: 0.6043\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 86s 535ms/step - loss: 0.4950 - acc: 0.7548 - val_loss: 0.7631 - val_acc: 0.5971\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 96s 602ms/step - loss: 0.4812 - acc: 0.7628 - val_loss: 0.7739 - val_acc: 0.5973\n",
      "200/200 [==============================] - 17s 79ms/step - loss: 0.9157 - acc: 0.5159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9156829118728638, 0.5159375071525574]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1[:2560] + lines3[:2560] + lines5[:2560] + lines7[:2560] + lines9[:2560]\n",
    "lines_ut = lines2[:2560] + lines4[:2560] + lines6[:2560] + lines8[:2560] + lines10[:2560]\n",
    "test_t = test1[:640] + test3[:640] + test5[:640] + test7[:640] + test9[:640]\n",
    "test_ut = test2[:640] + test4[:640] + test6[:640] + test8[:640] + test10[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c17ee6",
   "metadata": {},
   "source": [
    "## 완벽한 타인 + 암살"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c83a665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10240/10240 [01:53<00:00, 90.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2560/2560 [00:30<00:00, 85.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 57s 799ms/step - loss: 0.6854 - acc: 0.5588 - val_loss: 0.6734 - val_acc: 0.5923\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 49s 763ms/step - loss: 0.6370 - acc: 0.6454 - val_loss: 0.6608 - val_acc: 0.6162\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 52s 813ms/step - loss: 0.5917 - acc: 0.6843 - val_loss: 0.6769 - val_acc: 0.6064\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 51s 799ms/step - loss: 0.5562 - acc: 0.7192 - val_loss: 0.7201 - val_acc: 0.6011\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 45s 695ms/step - loss: 0.5284 - acc: 0.7356 - val_loss: 0.7274 - val_acc: 0.5981\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 36s 565ms/step - loss: 0.4972 - acc: 0.7562 - val_loss: 0.7472 - val_acc: 0.5913\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 33s 517ms/step - loss: 0.4737 - acc: 0.7701 - val_loss: 0.8076 - val_acc: 0.5796\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 48s 751ms/step - loss: 0.4514 - acc: 0.7842 - val_loss: 0.8267 - val_acc: 0.5815\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 52s 811ms/step - loss: 0.4354 - acc: 0.7938 - val_loss: 0.8511 - val_acc: 0.5718\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 52s 813ms/step - loss: 0.4138 - acc: 0.8040 - val_loss: 0.8806 - val_acc: 0.5728\n",
      "80/80 [==============================] - 7s 74ms/step - loss: 1.0436 - acc: 0.5043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0435749292373657, 0.5042968988418579]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines3[:2560] + lines5[:2560] \n",
    "lines_ut = lines4[:2560] + lines6[:2560]\n",
    "test_t = test3[:640] + test5[:640] \n",
    "test_ut = test4[:640] + test6[:640] \n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef26b5",
   "metadata": {},
   "source": [
    "## 완벽한 타인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ad9f956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5120/5120 [00:47<00:00, 107.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:11<00:00, 110.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 23s 578ms/step - loss: 0.6914 - acc: 0.5312 - val_loss: 0.6892 - val_acc: 0.5557\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 16s 511ms/step - loss: 0.6730 - acc: 0.6069 - val_loss: 0.6753 - val_acc: 0.5859\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 17s 516ms/step - loss: 0.6226 - acc: 0.6707 - val_loss: 0.6844 - val_acc: 0.5732\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 17s 530ms/step - loss: 0.6535 - acc: 0.6887 - val_loss: 0.6809 - val_acc: 0.5820\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 23s 727ms/step - loss: 0.5620 - acc: 0.7214 - val_loss: 0.6980 - val_acc: 0.5801\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 26s 819ms/step - loss: 0.5221 - acc: 0.7527 - val_loss: 0.7094 - val_acc: 0.5752\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 26s 801ms/step - loss: 0.4940 - acc: 0.7734 - val_loss: 0.7275 - val_acc: 0.5801\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 27s 844ms/step - loss: 0.4605 - acc: 0.7947 - val_loss: 0.7474 - val_acc: 0.5752\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 26s 805ms/step - loss: 0.4249 - acc: 0.8098 - val_loss: 0.8354 - val_acc: 0.5752\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 25s 788ms/step - loss: 1.0382 - acc: 0.7168 - val_loss: 0.8367 - val_acc: 0.5781\n",
      "40/40 [==============================] - 5s 83ms/step - loss: 0.9315 - acc: 0.5234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9314674139022827, 0.5234375]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras.models import Model\n",
    "lines_t = lines3[:2560]\n",
    "lines_ut = lines4[:2560]\n",
    "test_t = test3[:640]\n",
    "test_ut = test4[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effabdd3",
   "metadata": {},
   "source": [
    "## 암살"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90bc0ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5120/5120 [00:51<00:00, 99.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:10<00:00, 117.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 21s 563ms/step - loss: 0.6897 - acc: 0.5466 - val_loss: 0.6804 - val_acc: 0.5684\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 17s 519ms/step - loss: 0.6603 - acc: 0.6121 - val_loss: 0.6619 - val_acc: 0.6143\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 17s 521ms/step - loss: 0.6101 - acc: 0.6707 - val_loss: 0.6589 - val_acc: 0.6113\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 19s 581ms/step - loss: 0.5620 - acc: 0.7100 - val_loss: 0.6743 - val_acc: 0.6025\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 25s 772ms/step - loss: 0.5239 - acc: 0.7400 - val_loss: 0.6952 - val_acc: 0.5947\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 25s 786ms/step - loss: 1.6513 - acc: 0.5771 - val_loss: 2.2074 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 26s 818ms/step - loss: 1.8055 - acc: 0.5068 - val_loss: 0.9939 - val_acc: 0.4971\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 25s 793ms/step - loss: 0.6587 - acc: 0.6519 - val_loss: 0.7074 - val_acc: 0.5859\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 27s 834ms/step - loss: 0.5378 - acc: 0.7334 - val_loss: 0.6926 - val_acc: 0.5879\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 25s 784ms/step - loss: 0.4864 - acc: 0.7812 - val_loss: 0.7106 - val_acc: 0.5781\n",
      "40/40 [==============================] - 4s 79ms/step - loss: 0.7719 - acc: 0.5281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.771903932094574, 0.528124988079071]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines5[:2560]\n",
    "lines_ut = lines6[:2560]\n",
    "test_t = test5[:640]\n",
    "test_ut = test6[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91538ac",
   "metadata": {},
   "source": [
    "## 1987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d46026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5120/5120 [00:51<00:00, 99.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:09<00:00, 129.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 20s 548ms/step - loss: 0.6917 - acc: 0.5332 - val_loss: 0.6894 - val_acc: 0.5430\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 17s 524ms/step - loss: 0.6771 - acc: 0.5942 - val_loss: 0.6802 - val_acc: 0.5449\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 17s 519ms/step - loss: 0.8099 - acc: 0.6238 - val_loss: 1.4478 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 22s 674ms/step - loss: 1.0723 - acc: 0.5110 - val_loss: 0.9764 - val_acc: 0.5068\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 26s 815ms/step - loss: 0.7632 - acc: 0.5630 - val_loss: 0.7214 - val_acc: 0.5566\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 26s 818ms/step - loss: 0.6214 - acc: 0.6472 - val_loss: 0.6762 - val_acc: 0.5928\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 25s 789ms/step - loss: 0.5734 - acc: 0.7034 - val_loss: 0.6736 - val_acc: 0.5830\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 26s 814ms/step - loss: 0.5231 - acc: 0.7480 - val_loss: 0.6883 - val_acc: 0.5840\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 26s 804ms/step - loss: 0.4928 - acc: 0.7686 - val_loss: 0.7176 - val_acc: 0.5850\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 25s 790ms/step - loss: 0.4614 - acc: 0.7839 - val_loss: 0.7085 - val_acc: 0.5820\n",
      "40/40 [==============================] - 5s 83ms/step - loss: 0.7947 - acc: 0.5312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.794721782207489, 0.53125]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines7[:2560]\n",
    "lines_ut = lines8[:2560]\n",
    "test_t = test7[:640]\n",
    "test_ut = test8[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd14bb",
   "metadata": {},
   "source": [
    "## 택시운전사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d61d0f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5120/5120 [00:43<00:00, 117.41it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:07<00:00, 175.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 17s 467ms/step - loss: 0.6920 - acc: 0.5239 - val_loss: 0.6890 - val_acc: 0.5361\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 13s 419ms/step - loss: 0.6703 - acc: 0.6196 - val_loss: 0.6713 - val_acc: 0.5713\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 13s 417ms/step - loss: 0.6176 - acc: 0.6780 - val_loss: 0.6702 - val_acc: 0.5723\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 13s 417ms/step - loss: 0.5700 - acc: 0.7122 - val_loss: 0.6778 - val_acc: 0.5713\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 13s 421ms/step - loss: 0.5591 - acc: 0.7402 - val_loss: 0.6942 - val_acc: 0.5801\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 13s 418ms/step - loss: 0.4922 - acc: 0.7646 - val_loss: 0.7143 - val_acc: 0.5703\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 13s 416ms/step - loss: 0.4633 - acc: 0.7842 - val_loss: 0.7392 - val_acc: 0.5781\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 13s 417ms/step - loss: 0.4309 - acc: 0.8047 - val_loss: 0.7729 - val_acc: 0.5703\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 13s 417ms/step - loss: 0.4819 - acc: 0.7971 - val_loss: 0.7875 - val_acc: 0.5693\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 13s 419ms/step - loss: 0.7310 - acc: 0.7317 - val_loss: 1.3976 - val_acc: 0.5312\n",
      "40/40 [==============================] - 3s 57ms/step - loss: 1.5461 - acc: 0.5031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.54606032371521, 0.503125011920929]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines9[:2560]\n",
    "lines_ut = lines10[:2560]\n",
    "test_t = test9[:640]\n",
    "test_ut = test10[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    text = line.split(\",\")[3]\n",
    "    test_lines_.append(text.strip())\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "x_train = []\n",
    "x_test = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# train data\n",
    "for sentence in tqdm(lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_train.append(stopwords_removed_sentence)\n",
    "\n",
    "# test data\n",
    "for sentence in tqdm(test_lines_):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    x_test.append(stopwords_removed_sentence)\n",
    "\n",
    "maxlen = 1000  # 100개 단어 이후는 버립니다\n",
    "training_samples = 200  # 훈련 샘플은 200 -> 200개입니다\n",
    "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
    "# train 토큰화\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train = pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "# test 토큰화\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(x_test)\n",
    "sequences2 = tokenizer2.texts_to_sequences(x_test)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "test = pad_sequences(sequences2,maxlen=maxlen)\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "\n",
    "batch_size = 32\n",
    "input_train = sequence.pad_sequences(train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(input_train, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=10, mode='auto')\n",
    "\n",
    "\"\"\"history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,callbacks=[early_stopping]\n",
    "                    )\"\"\"\n",
    "\n",
    "history = model.fit(x_train, y_train,validation_data=(x_valid, y_valid),\n",
    "                    epochs=10,\n",
    "                    batch_size=128\n",
    "                    )\n",
    "\n",
    "model.evaluate(input_test, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "383.963px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
