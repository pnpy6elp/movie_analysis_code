{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908d9db2",
   "metadata": {},
   "source": [
    "# definition1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8d8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import codecs\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import nltk\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# train set\n",
    "name = \"1\"\n",
    "filename1 = './new_data/definition'+name+'_spynorth_scaling_trust.txt'\n",
    "filename2 = './new_data/definition'+name+'_spynorth_scaling_untrust.txt'\n",
    "filename3 = './new_data/definition'+name+'_intistranger_scaling_trust.txt'\n",
    "filename4 = './new_data/definition'+name+'_intistranger_scaling_untrust.txt'\n",
    "filename5 = './new_data/definition'+name+'_assassin_scaling_trust.txt'\n",
    "filename6 = './new_data/definition'+name+'_assassin_scaling_untrust.txt'\n",
    "filename7 = './new_data/definition'+name+'_1987_scaling_trust.txt'\n",
    "filename8 = './new_data/definition'+name+'_1987_scaling_untrust.txt'\n",
    "filename9 = './new_data/definition'+name+'_taxi_scaling_trust.txt'\n",
    "filename10 = './new_data/definition'+name+'_taxi_scaling_untrust.txt'\n",
    "filename11 = './new_data/definition'+name+'_gongjo_scaling_trust.txt'\n",
    "filename12 = './new_data/definition'+name+'_gongjo_scaling_untrust.txt'\n",
    "\n",
    "\n",
    "\n",
    "with codecs.open(filename1, 'r', encoding='utf-8-sig') as f:\n",
    "    lines1 = f.readlines()\n",
    "with codecs.open(filename2, 'r', encoding='utf-8-sig') as f:\n",
    "    lines2 = f.readlines()\n",
    "with codecs.open(filename3, 'r', encoding='utf-8-sig') as f:\n",
    "    lines3 = f.readlines()\n",
    "with codecs.open(filename4, 'r', encoding='utf-8-sig') as f:\n",
    "    lines4 = f.readlines()\n",
    "with codecs.open(filename5, 'r', encoding='utf-8-sig') as f:\n",
    "    lines5 = f.readlines()\n",
    "with codecs.open(filename6, 'r', encoding='utf-8-sig') as f:\n",
    "    lines6 = f.readlines()\n",
    "with codecs.open(filename7, 'r', encoding='utf-8-sig') as f:\n",
    "    lines7 = f.readlines()\n",
    "with codecs.open(filename8, 'r', encoding='utf-8-sig') as f:\n",
    "    lines8 = f.readlines()\n",
    "with codecs.open(filename9, 'r', encoding='utf-8-sig') as f:\n",
    "    lines9 = f.readlines()\n",
    "with codecs.open(filename10, 'r', encoding='utf-8-sig') as f:\n",
    "    lines10 = f.readlines()\n",
    "with codecs.open(filename9, 'r', encoding='utf-8-sig') as f:\n",
    "    lines11 = f.readlines()\n",
    "with codecs.open(filename10, 'r', encoding='utf-8-sig') as f:\n",
    "    lines12 = f.readlines()\n",
    "\n",
    "\n",
    "\n",
    "# test set\n",
    "\n",
    "\n",
    "with codecs.open('./new_data/definition'+name+'_spynorth_test_t.txt', 'r', 'utf-8-sig') as f:\n",
    "    test1 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_spynorth_test_ut.txt', 'r', 'utf-8-sig') as f:\n",
    "    test2 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_intistranger_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test3 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_intistranger_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test4 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_assassin_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test5 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_assassin_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test6 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_1987_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test7 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_1987_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test8 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_taxi_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test9 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_taxi_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test10 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_gongjo_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test11 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_gongjo_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test12 = f.readlines()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a749c",
   "metadata": {},
   "source": [
    "## lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0ad19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "40/40 [==============================] - 8s 5ms/step - loss: 0.6724 - acc: 0.6494\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6391 - acc: 0.7037\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6116 - acc: 0.7025\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5933 - acc: 0.7047\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5827 - acc: 0.7029\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5752 - acc: 0.7027\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5696 - acc: 0.7025\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5658 - acc: 0.7047\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5620 - acc: 0.7031\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5615 - acc: 0.7033\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5597 - acc: 0.7020\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5587 - acc: 0.7014\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5578 - acc: 0.7035\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5572 - acc: 0.7068\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5573 - acc: 0.7041\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5557 - acc: 0.7035\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5541 - acc: 0.7086\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5551 - acc: 0.7043\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5541 - acc: 0.7078\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5543 - acc: 0.7070\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5540 - acc: 0.7033\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5535 - acc: 0.7061\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5530 - acc: 0.7070\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5534 - acc: 0.7086\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5532 - acc: 0.7086\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5537 - acc: 0.7043\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5532 - acc: 0.7023\n",
      "training time : 14.112138\n",
      "40/40 [==============================] - 3s 3ms/step - loss: 0.5404 - acc: 0.7148\n",
      "test time : 2.819193\n",
      "total time: 16.931331\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 6s 5ms/step - loss: 0.6859 - acc: 0.5912\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6751 - acc: 0.6059\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6631 - acc: 0.6018\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6527 - acc: 0.5963\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6454 - acc: 0.5969\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6409 - acc: 0.5977\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6378 - acc: 0.6049\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6353 - acc: 0.6049\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6342 - acc: 0.6031\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6325 - acc: 0.6047\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6321 - acc: 0.6074\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6304 - acc: 0.6135\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6304 - acc: 0.6072\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6297 - acc: 0.6146\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6294 - acc: 0.6121\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6290 - acc: 0.6170\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6283 - acc: 0.6150\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6283 - acc: 0.6166\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6286 - acc: 0.6119\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6278 - acc: 0.6178\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6283 - acc: 0.6113\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6278 - acc: 0.6154\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6273 - acc: 0.6211\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6272 - acc: 0.6141\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6272 - acc: 0.6172\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6269 - acc: 0.6199\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6268 - acc: 0.6223\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6255 - acc: 0.6176\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6272 - acc: 0.6125\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6254 - acc: 0.6227\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6262 - acc: 0.6246\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6266 - acc: 0.6174\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6255 - acc: 0.6211\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6256 - acc: 0.6203\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6256 - acc: 0.6186\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6252 - acc: 0.6201\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6254 - acc: 0.6174\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6249 - acc: 0.6252\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6251 - acc: 0.6238\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6247 - acc: 0.6180\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6250 - acc: 0.6195\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6248 - acc: 0.6225\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6242 - acc: 0.6215\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6240 - acc: 0.6250\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6245 - acc: 0.6191\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6254 - acc: 0.6211\n",
      "Epoch 47/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6243 - acc: 0.6197\n",
      "Epoch 48/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6240 - acc: 0.6260\n",
      "Epoch 49/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6245 - acc: 0.6199\n",
      "Epoch 50/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6234 - acc: 0.6264\n",
      "Epoch 51/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6257 - acc: 0.6223\n",
      "Epoch 52/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6239 - acc: 0.6217\n",
      "Epoch 53/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6236 - acc: 0.6221\n",
      "Epoch 54/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6230 - acc: 0.6273\n",
      "Epoch 55/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6247 - acc: 0.6244\n",
      "Epoch 56/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6242 - acc: 0.6201\n",
      "Epoch 57/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6235 - acc: 0.6229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6236 - acc: 0.6252\n",
      "Epoch 59/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6230 - acc: 0.6217\n",
      "Epoch 60/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6231 - acc: 0.6244\n",
      "Epoch 61/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6233 - acc: 0.6201\n",
      "Epoch 62/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6243 - acc: 0.6227\n",
      "Epoch 63/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6232 - acc: 0.6268\n",
      "Epoch 64/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6237 - acc: 0.6205\n",
      "training time : 20.960212\n",
      "40/40 [==============================] - 2s 3ms/step - loss: 0.6129 - acc: 0.6313\n",
      "test time : 2.419492\n",
      "total time: 23.379703\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 6s 5ms/step - loss: 0.6896 - acc: 0.5496\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6834 - acc: 0.5891\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6754 - acc: 0.5955\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6664 - acc: 0.5979\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6591 - acc: 0.5926\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6537 - acc: 0.5928\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6497 - acc: 0.5961\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6470 - acc: 0.6010\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6446 - acc: 0.6008\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6430 - acc: 0.5973\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6409 - acc: 0.5975\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6395 - acc: 0.5990\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6387 - acc: 0.6043\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6378 - acc: 0.6053\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6375 - acc: 0.5992\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6369 - acc: 0.6053\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6360 - acc: 0.6029\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6355 - acc: 0.6062\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6356 - acc: 0.6059\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6356 - acc: 0.6053\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6345 - acc: 0.6027\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6347 - acc: 0.6031\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6346 - acc: 0.6041\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6345 - acc: 0.6037\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6338 - acc: 0.6047\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6343 - acc: 0.6012\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6334 - acc: 0.6047\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6339 - acc: 0.6061\n",
      "training time : 12.290875\n",
      "40/40 [==============================] - 3s 3ms/step - loss: 0.6405 - acc: 0.6102\n",
      "test time : 3.151248\n",
      "total time: 15.442123\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 7s 6ms/step - loss: 0.6919 - acc: 0.5404\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6884 - acc: 0.5586\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6835 - acc: 0.5602\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6772 - acc: 0.5590\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6710 - acc: 0.5527\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6662 - acc: 0.5529\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6628 - acc: 0.5580\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6625 - acc: 0.5523\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6611 - acc: 0.5604\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6604 - acc: 0.5660\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6598 - acc: 0.5658\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6594 - acc: 0.5660\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6599 - acc: 0.5648\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6596 - acc: 0.5607\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6596 - acc: 0.5691\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6583 - acc: 0.5734\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6585 - acc: 0.5701\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6595 - acc: 0.5666\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6590 - acc: 0.5684\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6587 - acc: 0.5705\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6585 - acc: 0.5668\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6580 - acc: 0.5715\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6585 - acc: 0.5701\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6576 - acc: 0.5658\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6581 - acc: 0.5672\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6584 - acc: 0.5736\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6580 - acc: 0.5725\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6581 - acc: 0.5730\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6578 - acc: 0.5686\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6575 - acc: 0.5732\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6578 - acc: 0.5723\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6580 - acc: 0.5715\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6576 - acc: 0.5705\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6574 - acc: 0.5697\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6577 - acc: 0.5742\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6577 - acc: 0.5703\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6570 - acc: 0.5801\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6567 - acc: 0.5711\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6570 - acc: 0.5771\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6571 - acc: 0.5738\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6573 - acc: 0.5752\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6572 - acc: 0.5752\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6569 - acc: 0.5785\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6571 - acc: 0.5725\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6571 - acc: 0.5738\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6573 - acc: 0.5723\n",
      "Epoch 47/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6569 - acc: 0.5732\n",
      "training time : 16.267665\n",
      "40/40 [==============================] - 2s 2ms/step - loss: 0.6491 - acc: 0.5906\n",
      "test time : 2.234886\n",
      "total time: 18.502551\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 7s 2ms/step - loss: 0.6867 - acc: 0.5688\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6756 - acc: 0.5941\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6633 - acc: 0.5973\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6529 - acc: 0.5881\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6466 - acc: 0.5910\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6439 - acc: 0.5857\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6423 - acc: 0.5855\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6409 - acc: 0.5852\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6403 - acc: 0.5889\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6394 - acc: 0.5949\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6395 - acc: 0.5920\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6392 - acc: 0.5975\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6391 - acc: 0.5934\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6391 - acc: 0.5957\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6383 - acc: 0.5943\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6386 - acc: 0.5996\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6384 - acc: 0.5951\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6380 - acc: 0.5973\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6382 - acc: 0.5973\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6377 - acc: 0.5977\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6379 - acc: 0.5953\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6380 - acc: 0.5947\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6378 - acc: 0.5988\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6373 - acc: 0.5996\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6378 - acc: 0.5957\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6378 - acc: 0.5982\n",
      "training time : 10.541435\n",
      "40/40 [==============================] - 3s 3ms/step - loss: 0.6389 - acc: 0.5969\n",
      "test time : 3.068249\n",
      "total time: 13.609684\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 6s 5ms/step - loss: 0.6879 - acc: 0.5732\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6801 - acc: 0.5979\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6696 - acc: 0.5934\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6576 - acc: 0.5918\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6502 - acc: 0.5787\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6455 - acc: 0.5857\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6435 - acc: 0.5871\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6426 - acc: 0.5850\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6410 - acc: 0.5857\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6403 - acc: 0.5859\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6398 - acc: 0.5875\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6393 - acc: 0.5877\n",
      "training time : 8.929166\n",
      "40/40 [==============================] - 3s 2ms/step - loss: 0.5948 - acc: 0.6516\n",
      "test time : 2.597674\n",
      "total time: 11.526840\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import time\n",
    "lines_t = lines1[:3200]\n",
    "lines_ut = lines2[:3200]\n",
    "test_t = test1[:800]\n",
    "test_ut = test2[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines3[:3200]\n",
    "lines_ut = lines4[:3200]\n",
    "test_t = test3[:800]\n",
    "test_ut = test4[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines5[:3200]\n",
    "lines_ut = lines6[:3200]\n",
    "test_t = test5[:800]\n",
    "test_ut = test6[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines7[:3200]\n",
    "lines_ut = lines8[:3200]\n",
    "test_t = test7[:800]\n",
    "test_ut = test8[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines9[:3200]\n",
    "lines_ut = lines10[:3200]\n",
    "test_t = test9[:800]\n",
    "test_ut = test10[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines11[:3200]\n",
    "lines_ut = lines12[:3200]\n",
    "test_t = test11[:800]\n",
    "test_ut = test12[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed2f92f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "13.8502485\n",
      "test\n",
      "2.7151236666666665\n",
      "total\n",
      "16.565372\n"
     ]
    }
   ],
   "source": [
    "print(\"train\")\n",
    "print((14.112138+20.960212+12.290875+16.267665+10.541435+8.929166)/6)\n",
    "print(\"test\")\n",
    "print((2.819193+2.419492+3.151248+2.234886+3.068249+2.597674)/6)\n",
    "print(\"total\")\n",
    "print((16.931331+23.379703+15.442123+18.502551+13.609684+11.526840)/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1d31a",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe27c46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time : 4.725416\n",
      "0.72578125\n",
      "test time : 0.937658\n",
      "total time: 5.663074\n",
      "training time : 3.270568\n",
      "0.628125\n",
      "test time : 1.162313\n",
      "total time: 4.432881\n",
      "training time : 3.301143\n",
      "0.62421875\n",
      "test time : 1.249919\n",
      "total time: 4.551062\n",
      "training time : 3.144940\n",
      "0.59921875\n",
      "test time : 1.300493\n",
      "total time: 4.445433\n",
      "training time : 3.244737\n",
      "0.60859375\n",
      "test time : 1.254452\n",
      "total time: 4.499189\n",
      "training time : 3.245855\n",
      "0.6796875\n",
      "test time : 1.254652\n",
      "total time: 4.500507\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "lines_t = lines1[:2560]\n",
    "lines_ut = lines2[:2560]\n",
    "test_t = test1[:640]\n",
    "test_ut = test2[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "####################################################\n",
    "\n",
    "lines_t = lines3[:2560]\n",
    "lines_ut = lines4[:2560]\n",
    "test_t = test3[:640]\n",
    "test_ut = test4[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "##############################\n",
    "lines_t = lines5[:2560]\n",
    "lines_ut = lines6[:2560]\n",
    "test_t = test5[:640]\n",
    "test_ut = test6[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "################################\n",
    "lines_t = lines7[:2560]\n",
    "lines_ut = lines8[:2560]\n",
    "test_t = test7[:640]\n",
    "test_ut = test8[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "########################################\n",
    "lines_t = lines9[:2560]\n",
    "lines_ut = lines10[:2560]\n",
    "test_t = test9[:640]\n",
    "test_ut = test10[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "#########################\n",
    "lines_t = lines11[:2560]\n",
    "lines_ut = lines12[:2560]\n",
    "test_t = test11[:640]\n",
    "test_ut = test12[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "594b5568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "3.4887764999999997\n",
      "test\n",
      "1.1932478333333332\n",
      "total\n",
      "4.682024333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"train\")\n",
    "print((4.725416+3.270568+3.301143+3.144940+3.244737+3.245855)/6)\n",
    "print(\"test\")\n",
    "print((0.937658+1.162313+1.249919+1.300493+1.254452+1.254652)/6)\n",
    "print(\"total\")\n",
    "print((5.663074+4.432881+4.551062+4.445433+4.499189+4.500507)/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0796614",
   "metadata": {},
   "source": [
    "# definition 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9d25d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import codecs\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import nltk\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# train set\n",
    "name = \"2\"\n",
    "filename1 = './new_data/definition'+name+'_spynorth_scaling_trust.txt'\n",
    "filename2 = './new_data/definition'+name+'_spynorth_scaling_untrust.txt'\n",
    "filename3 = './new_data/definition'+name+'_intistranger_scaling_trust.txt'\n",
    "filename4 = './new_data/definition'+name+'_intistranger_scaling_untrust.txt'\n",
    "filename5 = './new_data/definition'+name+'_assassin_scaling_trust.txt'\n",
    "filename6 = './new_data/definition'+name+'_assassin_scaling_untrust.txt'\n",
    "filename7 = './new_data/definition'+name+'_1987_scaling_trust.txt'\n",
    "filename8 = './new_data/definition'+name+'_1987_scaling_untrust.txt'\n",
    "filename9 = './new_data/definition'+name+'_taxi_scaling_trust.txt'\n",
    "filename10 = './new_data/definition'+name+'_taxi_scaling_untrust.txt'\n",
    "filename11 = './new_data/definition'+name+'_gongjo_scaling_trust.txt'\n",
    "filename12 = './new_data/definition'+name+'_gongjo_scaling_untrust.txt'\n",
    "\n",
    "\n",
    "\n",
    "with codecs.open(filename1, 'r', encoding='utf-8-sig') as f:\n",
    "    lines1 = f.readlines()\n",
    "with codecs.open(filename2, 'r', encoding='utf-8-sig') as f:\n",
    "    lines2 = f.readlines()\n",
    "with codecs.open(filename3, 'r', encoding='utf-8-sig') as f:\n",
    "    lines3 = f.readlines()\n",
    "with codecs.open(filename4, 'r', encoding='utf-8-sig') as f:\n",
    "    lines4 = f.readlines()\n",
    "with codecs.open(filename5, 'r', encoding='utf-8-sig') as f:\n",
    "    lines5 = f.readlines()\n",
    "with codecs.open(filename6, 'r', encoding='utf-8-sig') as f:\n",
    "    lines6 = f.readlines()\n",
    "with codecs.open(filename7, 'r', encoding='utf-8-sig') as f:\n",
    "    lines7 = f.readlines()\n",
    "with codecs.open(filename8, 'r', encoding='utf-8-sig') as f:\n",
    "    lines8 = f.readlines()\n",
    "with codecs.open(filename9, 'r', encoding='utf-8-sig') as f:\n",
    "    lines9 = f.readlines()\n",
    "with codecs.open(filename10, 'r', encoding='utf-8-sig') as f:\n",
    "    lines10 = f.readlines()\n",
    "with codecs.open(filename9, 'r', encoding='utf-8-sig') as f:\n",
    "    lines11 = f.readlines()\n",
    "with codecs.open(filename10, 'r', encoding='utf-8-sig') as f:\n",
    "    lines12 = f.readlines()\n",
    "\n",
    "\n",
    "\n",
    "# test set\n",
    "\n",
    "\n",
    "with codecs.open('./new_data/definition'+name+'_spynorth_test_t.txt', 'r', 'utf-8-sig') as f:\n",
    "    test1 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_spynorth_test_ut.txt', 'r', 'utf-8-sig') as f:\n",
    "    test2 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_intistranger_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test3 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_intistranger_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test4 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_assassin_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test5 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_assassin_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test6 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_1987_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test7 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_1987_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test8 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_taxi_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test9 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_taxi_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test10 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_gongjo_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test11 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_gongjo_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test12 = f.readlines()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe7cce",
   "metadata": {},
   "source": [
    "## lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41f056cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "40/40 [==============================] - 7s 5ms/step - loss: 0.6920 - acc: 0.5311\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6911 - acc: 0.5389\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6902 - acc: 0.5453\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6893 - acc: 0.5480\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6884 - acc: 0.5492\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6876 - acc: 0.5521\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6867 - acc: 0.5594\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6865 - acc: 0.5664\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6854 - acc: 0.5609\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6850 - acc: 0.5656\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6844 - acc: 0.5578\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6838 - acc: 0.5529\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6830 - acc: 0.5584\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6823 - acc: 0.5561\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6820 - acc: 0.5615\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6812 - acc: 0.5615\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6805 - acc: 0.5625\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6800 - acc: 0.5656\n",
      "training time : 10.580416\n",
      "40/40 [==============================] - 2s 3ms/step - loss: 0.6817 - acc: 0.5523\n",
      "test time : 2.485059\n",
      "total time: 13.065475\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 7s 5ms/step - loss: 0.6930 - acc: 0.5168\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6926 - acc: 0.5148\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6921 - acc: 0.5148\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6916 - acc: 0.5199\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6916 - acc: 0.5178\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6913 - acc: 0.5160\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6911 - acc: 0.5207\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6909 - acc: 0.5207\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6909 - acc: 0.5250\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6905 - acc: 0.5256\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6904 - acc: 0.5242\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6898 - acc: 0.5199\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6896 - acc: 0.5207\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6889 - acc: 0.5248\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6890 - acc: 0.5271\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6877 - acc: 0.5277\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6875 - acc: 0.5338\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6874 - acc: 0.5287\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6874 - acc: 0.5252\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6868 - acc: 0.5373\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6859 - acc: 0.5352\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6864 - acc: 0.5295\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6860 - acc: 0.5314\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6854 - acc: 0.5291\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6851 - acc: 0.5346\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6850 - acc: 0.5346\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6850 - acc: 0.5377\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6836 - acc: 0.5363\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6840 - acc: 0.5434\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6839 - acc: 0.5426\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6832 - acc: 0.5463\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6823 - acc: 0.5488\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6825 - acc: 0.5439\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6822 - acc: 0.5584\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6818 - acc: 0.5521\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6816 - acc: 0.5566\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6814 - acc: 0.5498\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6807 - acc: 0.5613\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6812 - acc: 0.5582\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6805 - acc: 0.5619\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6801 - acc: 0.5639\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6799 - acc: 0.5578\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6795 - acc: 0.5635\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6785 - acc: 0.5574\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6784 - acc: 0.5664\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6782 - acc: 0.5621\n",
      "Epoch 47/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6778 - acc: 0.5656\n",
      "Epoch 48/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6782 - acc: 0.5742\n",
      "Epoch 49/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6783 - acc: 0.5686\n",
      "Epoch 50/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6780 - acc: 0.5658\n",
      "Epoch 51/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6779 - acc: 0.5654\n",
      "Epoch 52/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6774 - acc: 0.5729\n",
      "Epoch 53/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6773 - acc: 0.5688\n",
      "Epoch 54/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6763 - acc: 0.5725\n",
      "Epoch 55/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6773 - acc: 0.5660\n",
      "Epoch 56/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6771 - acc: 0.5736\n",
      "Epoch 57/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6767 - acc: 0.5721\n",
      "Epoch 58/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6760 - acc: 0.5723\n",
      "training time : 19.625252\n",
      "40/40 [==============================] - 2s 2ms/step - loss: 0.6784 - acc: 0.5648\n",
      "test time : 3.066136\n",
      "total time: 22.691389\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 7s 5ms/step - loss: 0.6931 - acc: 0.5086\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6926 - acc: 0.5189\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6919 - acc: 0.5299\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6911 - acc: 0.5496\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6896 - acc: 0.5500\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6878 - acc: 0.5523\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6861 - acc: 0.5504\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6846 - acc: 0.5475\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6839 - acc: 0.5494\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6837 - acc: 0.5484\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6834 - acc: 0.5510\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6833 - acc: 0.5531\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6828 - acc: 0.5490\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6831 - acc: 0.5473\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6828 - acc: 0.5500\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6827 - acc: 0.5502\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6826 - acc: 0.5518\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6814 - acc: 0.5529\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6821 - acc: 0.5527\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6819 - acc: 0.5527\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6816 - acc: 0.5561\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6813 - acc: 0.5502\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6813 - acc: 0.5529\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6809 - acc: 0.5506\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6809 - acc: 0.5543\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6806 - acc: 0.5574\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6800 - acc: 0.5559\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6800 - acc: 0.5537\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6795 - acc: 0.5578\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6795 - acc: 0.5602\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6790 - acc: 0.5602\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6789 - acc: 0.5607\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6785 - acc: 0.5621\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6785 - acc: 0.5662\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6780 - acc: 0.5645\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6780 - acc: 0.5645\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6776 - acc: 0.5652\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6777 - acc: 0.5676\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6774 - acc: 0.5676\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6772 - acc: 0.5680\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6774 - acc: 0.5658\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6774 - acc: 0.5648\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6771 - acc: 0.5646\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6770 - acc: 0.5658\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6767 - acc: 0.5643\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6769 - acc: 0.5625\n",
      "Epoch 47/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6771 - acc: 0.5662\n",
      "Epoch 48/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6768 - acc: 0.5688\n",
      "Epoch 49/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6771 - acc: 0.5650\n",
      "Epoch 50/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6768 - acc: 0.5646\n",
      "Epoch 51/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6769 - acc: 0.5660\n",
      "Epoch 52/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6770 - acc: 0.5660\n",
      "Epoch 53/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6770 - acc: 0.5658\n",
      "Epoch 54/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6769 - acc: 0.5643\n",
      "Epoch 55/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6768 - acc: 0.5639\n",
      "Epoch 56/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6768 - acc: 0.5662\n",
      "Epoch 57/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6768 - acc: 0.5658\n",
      "Epoch 58/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6770 - acc: 0.5656\n",
      "training time : 20.716184\n",
      "40/40 [==============================] - 2s 3ms/step - loss: 0.6816 - acc: 0.5562\n",
      "test time : 2.517847\n",
      "total time: 23.234031\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 7s 6ms/step - loss: 0.6931 - acc: 0.5055\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6919 - acc: 0.5398\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6904 - acc: 0.5369\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6889 - acc: 0.5498\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6873 - acc: 0.5457\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6863 - acc: 0.5447\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6858 - acc: 0.5375\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6853 - acc: 0.5477\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6855 - acc: 0.5455\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6853 - acc: 0.5434\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6849 - acc: 0.5441\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6847 - acc: 0.5447\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6850 - acc: 0.5426\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6847 - acc: 0.5412\n",
      "training time : 10.660876\n",
      "40/40 [==============================] - 3s 3ms/step - loss: 0.6872 - acc: 0.5367\n",
      "test time : 2.704274\n",
      "total time: 13.365149\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 7s 6ms/step - loss: 0.6929 - acc: 0.5188\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6913 - acc: 0.5420\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6890 - acc: 0.5479\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6860 - acc: 0.5572\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6832 - acc: 0.5678\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6809 - acc: 0.5621\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6794 - acc: 0.5660\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6791 - acc: 0.5684\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6793 - acc: 0.5580\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6792 - acc: 0.5594\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6792 - acc: 0.5631\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6786 - acc: 0.5598\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6784 - acc: 0.5637\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6784 - acc: 0.5629\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6782 - acc: 0.5631\n",
      "Epoch 16/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6779 - acc: 0.5670\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6782 - acc: 0.5637\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6779 - acc: 0.5684\n",
      "training time : 10.433910\n",
      "40/40 [==============================] - 2s 2ms/step - loss: 0.6774 - acc: 0.5602\n",
      "test time : 2.506050\n",
      "total time: 12.939960\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 7s 5ms/step - loss: 0.6930 - acc: 0.5029\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6915 - acc: 0.5320\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6893 - acc: 0.5584\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6868 - acc: 0.5654\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6830 - acc: 0.5680\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6812 - acc: 0.5561\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6799 - acc: 0.5615\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6793 - acc: 0.5633\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6789 - acc: 0.5668\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6795 - acc: 0.5623\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6788 - acc: 0.5635\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6789 - acc: 0.5664\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6789 - acc: 0.5658\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6786 - acc: 0.5617\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6782 - acc: 0.5598\n",
      "training time : 10.565431\n",
      "40/40 [==============================] - 2s 3ms/step - loss: 0.6862 - acc: 0.5328\n",
      "test time : 2.471095\n",
      "total time: 13.036525\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1[:3200]\n",
    "lines_ut = lines2[:3200]\n",
    "test_t = test1[:800]\n",
    "test_ut = test2[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines3[:3200]\n",
    "lines_ut = lines4[:3200]\n",
    "test_t = test3[:800]\n",
    "test_ut = test4[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines5[:3200]\n",
    "lines_ut = lines6[:3200]\n",
    "test_t = test5[:800]\n",
    "test_ut = test6[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines7[:3200]\n",
    "lines_ut = lines8[:3200]\n",
    "test_t = test7[:800]\n",
    "test_ut = test8[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines9[:3200]\n",
    "lines_ut = lines10[:3200]\n",
    "test_t = test9[:800]\n",
    "test_ut = test10[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines11[:3200]\n",
    "lines_ut = lines12[:3200]\n",
    "test_t = test11[:800]\n",
    "test_ut = test12[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed71bdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "13.763678166666667\n",
      "test\n",
      "2.6250768333333334\n",
      "total\n",
      "16.388754833333333\n"
     ]
    }
   ],
   "source": [
    "print(\"train\")\n",
    "print((10.580416+19.625252+20.716184+10.660876+10.433910+10.565431)/6)\n",
    "print(\"test\")\n",
    "print((2.485059+3.066136+2.517847+2.704274+2.506050+ 2.471095)/6)\n",
    "print(\"total\")\n",
    "print((13.065475+22.691389+23.234031+13.365149+12.939960+13.036525)/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d5a4a6",
   "metadata": {},
   "source": [
    "## svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96cdfc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time : 4.295378\n",
      "0.5890625\n",
      "test time : 1.414005\n",
      "total time: 5.709383\n",
      "training time : 3.498927\n",
      "0.55390625\n",
      "test time : 1.412485\n",
      "total time: 4.911412\n",
      "training time : 3.347880\n",
      "0.5640625\n",
      "test time : 1.360104\n",
      "total time: 4.707984\n",
      "training time : 3.528563\n",
      "0.5609375\n",
      "test time : 1.371952\n",
      "total time: 4.900515\n",
      "training time : 3.296492\n",
      "0.5734375\n",
      "test time : 1.348435\n",
      "total time: 4.644927\n",
      "training time : 3.330062\n",
      "0.56796875\n",
      "test time : 1.431374\n",
      "total time: 4.761436\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "lines_t = lines1[:2560]\n",
    "lines_ut = lines2[:2560]\n",
    "test_t = test1[:640]\n",
    "test_ut = test2[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "####################################################\n",
    "\n",
    "lines_t = lines3[:2560]\n",
    "lines_ut = lines4[:2560]\n",
    "test_t = test3[:640]\n",
    "test_ut = test4[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "##############################\n",
    "lines_t = lines5[:2560]\n",
    "lines_ut = lines6[:2560]\n",
    "test_t = test5[:640]\n",
    "test_ut = test6[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "################################\n",
    "lines_t = lines7[:2560]\n",
    "lines_ut = lines8[:2560]\n",
    "test_t = test7[:640]\n",
    "test_ut = test8[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "########################################\n",
    "lines_t = lines9[:2560]\n",
    "lines_ut = lines10[:2560]\n",
    "test_t = test9[:640]\n",
    "test_ut = test10[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "#########################\n",
    "lines_t = lines11[:2560]\n",
    "lines_ut = lines12[:2560]\n",
    "test_t = test11[:640]\n",
    "test_ut = test12[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec911ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5495504333333336\n",
      "1.3897258333333333\n",
      "4.939276166666667\n"
     ]
    }
   ],
   "source": [
    "print((4.295378+3.498927+3.3478806+3.528563+3.296492+3.330062)/6)\n",
    "print((1.414005+1.412485+1.360104+1.371952+1.348435+1.431374)/6)\n",
    "print((5.709383+4.911412+4.707984+4.900515+4.644927+4.761436)/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c332555",
   "metadata": {},
   "source": [
    "# definition 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f0c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import codecs\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import nltk\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# train set\n",
    "name = \"3\"\n",
    "filename1 = './new_data/definition'+name+'_spynorth_scaling_trust.txt'\n",
    "filename2 = './new_data/definition'+name+'_spynorth_scaling_untrust.txt'\n",
    "filename3 = './new_data/definition'+name+'_intistranger_scaling_trust.txt'\n",
    "filename4 = './new_data/definition'+name+'_intistranger_scaling_untrust.txt'\n",
    "filename5 = './new_data/definition'+name+'_assassin_scaling_trust.txt'\n",
    "filename6 = './new_data/definition'+name+'_assassin_scaling_untrust.txt'\n",
    "filename7 = './new_data/definition'+name+'_1987_scaling_trust.txt'\n",
    "filename8 = './new_data/definition'+name+'_1987_scaling_untrust.txt'\n",
    "filename9 = './new_data/definition'+name+'_taxi_scaling_trust.txt'\n",
    "filename10 = './new_data/definition'+name+'_taxi_scaling_untrust.txt'\n",
    "filename11 = './new_data/definition'+name+'_gongjo_scaling_trust.txt'\n",
    "filename12 = './new_data/definition'+name+'_gongjo_scaling_untrust.txt'\n",
    "\n",
    "\n",
    "\n",
    "with codecs.open(filename1, 'r', encoding='utf-8-sig') as f:\n",
    "    lines1 = f.readlines()\n",
    "with codecs.open(filename2, 'r', encoding='utf-8-sig') as f:\n",
    "    lines2 = f.readlines()\n",
    "with codecs.open(filename3, 'r', encoding='utf-8-sig') as f:\n",
    "    lines3 = f.readlines()\n",
    "with codecs.open(filename4, 'r', encoding='utf-8-sig') as f:\n",
    "    lines4 = f.readlines()\n",
    "with codecs.open(filename5, 'r', encoding='utf-8-sig') as f:\n",
    "    lines5 = f.readlines()\n",
    "with codecs.open(filename6, 'r', encoding='utf-8-sig') as f:\n",
    "    lines6 = f.readlines()\n",
    "with codecs.open(filename7, 'r', encoding='utf-8-sig') as f:\n",
    "    lines7 = f.readlines()\n",
    "with codecs.open(filename8, 'r', encoding='utf-8-sig') as f:\n",
    "    lines8 = f.readlines()\n",
    "with codecs.open(filename9, 'r', encoding='utf-8-sig') as f:\n",
    "    lines9 = f.readlines()\n",
    "with codecs.open(filename10, 'r', encoding='utf-8-sig') as f:\n",
    "    lines10 = f.readlines()\n",
    "with codecs.open(filename9, 'r', encoding='utf-8-sig') as f:\n",
    "    lines11 = f.readlines()\n",
    "with codecs.open(filename10, 'r', encoding='utf-8-sig') as f:\n",
    "    lines12 = f.readlines()\n",
    "\n",
    "\n",
    "\n",
    "# test set\n",
    "\n",
    "\n",
    "with codecs.open('./new_data/definition'+name+'_spynorth_test_t.txt', 'r', 'utf-8-sig') as f:\n",
    "    test1 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_spynorth_test_ut.txt', 'r', 'utf-8-sig') as f:\n",
    "    test2 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_intistranger_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test3 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_intistranger_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test4 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_assassin_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test5 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_assassin_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test6 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_1987_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test7 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_1987_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test8 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_taxi_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test9 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_taxi_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test10 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_gongjo_test_t.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test11 = f.readlines()\n",
    "with codecs.open('./new_data/definition'+name+'_gongjo_test_ut.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    test12 = f.readlines()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0cded",
   "metadata": {},
   "source": [
    "## lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62087a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "40/40 [==============================] - 7s 5ms/step - loss: 0.6909 - acc: 0.5404\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6882 - acc: 0.5555\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6861 - acc: 0.5602\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6849 - acc: 0.5596\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6842 - acc: 0.5607\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6835 - acc: 0.5562\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6827 - acc: 0.5545\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6817 - acc: 0.5541\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6814 - acc: 0.5609\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6807 - acc: 0.5590\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6802 - acc: 0.5549\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6803 - acc: 0.5562\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6790 - acc: 0.5514\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6794 - acc: 0.5566\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6788 - acc: 0.5564\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6785 - acc: 0.5578\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6780 - acc: 0.5592\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6784 - acc: 0.5539\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6785 - acc: 0.5561\n",
      "training time : 11.749176\n",
      "40/40 [==============================] - 3s 3ms/step - loss: 0.6737 - acc: 0.5742\n",
      "test time : 2.645432\n",
      "total time: 14.394608\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 7s 6ms/step - loss: 0.6932 - acc: 0.5059\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6924 - acc: 0.5172\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6918 - acc: 0.5307\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6912 - acc: 0.5240\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6904 - acc: 0.5322\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6893 - acc: 0.5260\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6880 - acc: 0.5400\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6878 - acc: 0.5332\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6868 - acc: 0.5355\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6870 - acc: 0.5359\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6862 - acc: 0.5363\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6863 - acc: 0.5359\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6858 - acc: 0.5439\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6858 - acc: 0.5408\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6856 - acc: 0.5428\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6855 - acc: 0.5461\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6858 - acc: 0.5414\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6848 - acc: 0.5445\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6857 - acc: 0.5428\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6852 - acc: 0.5469\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6851 - acc: 0.5408\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6849 - acc: 0.5428\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6848 - acc: 0.5418\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6848 - acc: 0.5459\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6850 - acc: 0.5441\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6838 - acc: 0.5518\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6844 - acc: 0.5430\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6844 - acc: 0.5463\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6842 - acc: 0.5461\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6843 - acc: 0.5422\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6837 - acc: 0.5432\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6842 - acc: 0.5500\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6843 - acc: 0.5424\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6847 - acc: 0.5424\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6842 - acc: 0.5482\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6840 - acc: 0.5492\n",
      "training time : 15.073475\n",
      "40/40 [==============================] - 2s 3ms/step - loss: 0.6816 - acc: 0.5383\n",
      "test time : 2.406204\n",
      "total time: 17.479679\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 7s 5ms/step - loss: 0.6933 - acc: 0.5045\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6927 - acc: 0.5047\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6924 - acc: 0.5258\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6918 - acc: 0.5197\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6913 - acc: 0.5213\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6910 - acc: 0.5240\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6905 - acc: 0.5246\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6906 - acc: 0.5273\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6897 - acc: 0.5170\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6900 - acc: 0.5258\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6894 - acc: 0.5240\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6892 - acc: 0.5260\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6888 - acc: 0.5303\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6888 - acc: 0.5266\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6888 - acc: 0.5248\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6881 - acc: 0.5387\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6888 - acc: 0.5320\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6881 - acc: 0.5320\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6885 - acc: 0.5338\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6887 - acc: 0.5270\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6882 - acc: 0.5332\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6887 - acc: 0.5248\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6882 - acc: 0.5393\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6879 - acc: 0.5332\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6882 - acc: 0.5324\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6882 - acc: 0.5303\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6881 - acc: 0.5289\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6882 - acc: 0.5260\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6879 - acc: 0.5312\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6878 - acc: 0.5348\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6877 - acc: 0.5381\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6873 - acc: 0.5400\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6881 - acc: 0.5363\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6881 - acc: 0.5281\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6876 - acc: 0.5303\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6875 - acc: 0.5367\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6875 - acc: 0.5369\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6877 - acc: 0.5344\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6872 - acc: 0.5377\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6875 - acc: 0.5291\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6873 - acc: 0.5381\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6872 - acc: 0.5395\n",
      "training time : 16.789660\n",
      "40/40 [==============================] - 2s 3ms/step - loss: 0.6802 - acc: 0.5477\n",
      "test time : 2.530867\n",
      "total time: 19.320527\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 6s 6ms/step - loss: 0.6930 - acc: 0.5193\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6922 - acc: 0.5199\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6918 - acc: 0.5262\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6913 - acc: 0.5277\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6899 - acc: 0.5414\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6898 - acc: 0.5238\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6893 - acc: 0.5330\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6886 - acc: 0.5391\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6885 - acc: 0.5400\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6885 - acc: 0.5363\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6885 - acc: 0.5309\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6880 - acc: 0.5447\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6882 - acc: 0.5346\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6879 - acc: 0.5371\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6883 - acc: 0.5404\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6881 - acc: 0.5391\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6880 - acc: 0.5385\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6879 - acc: 0.5422\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6879 - acc: 0.5377\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6879 - acc: 0.5363\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6878 - acc: 0.5367\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6878 - acc: 0.5416\n",
      "training time : 11.420784\n",
      "40/40 [==============================] - 2s 3ms/step - loss: 0.6860 - acc: 0.5250\n",
      "test time : 2.515663\n",
      "total time: 13.936447\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 7s 5ms/step - loss: 0.6931 - acc: 0.5121\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6924 - acc: 0.5146\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6920 - acc: 0.5197\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6913 - acc: 0.5199\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6907 - acc: 0.5299\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6898 - acc: 0.5312\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6892 - acc: 0.5354\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6895 - acc: 0.5406\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6884 - acc: 0.5404\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6887 - acc: 0.5352\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6876 - acc: 0.5420\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6888 - acc: 0.5412\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6883 - acc: 0.5363\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6887 - acc: 0.5322\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6883 - acc: 0.5404\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6884 - acc: 0.5424\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6877 - acc: 0.5414\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6877 - acc: 0.5400\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6880 - acc: 0.5381\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6879 - acc: 0.5381\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6880 - acc: 0.5461\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6877 - acc: 0.5432\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6875 - acc: 0.5410\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6874 - acc: 0.5352\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6872 - acc: 0.5406\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6870 - acc: 0.5410\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6868 - acc: 0.5475\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6872 - acc: 0.5426\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6872 - acc: 0.5418\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6870 - acc: 0.5459\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6867 - acc: 0.5467\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6866 - acc: 0.5398\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6868 - acc: 0.5443\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6865 - acc: 0.5459\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6865 - acc: 0.5432\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6866 - acc: 0.5447\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6867 - acc: 0.5424\n",
      "training time : 15.883193\n",
      "40/40 [==============================] - 3s 3ms/step - loss: 0.6883 - acc: 0.5273\n",
      "test time : 2.564748\n",
      "total time: 18.447941\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 7s 6ms/step - loss: 0.6931 - acc: 0.5092\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6924 - acc: 0.5158\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6919 - acc: 0.5139\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6910 - acc: 0.5213\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6906 - acc: 0.5281\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6903 - acc: 0.5234\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6895 - acc: 0.5307\n",
      "Epoch 8/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6892 - acc: 0.5398\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6888 - acc: 0.5387\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6888 - acc: 0.5328\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6888 - acc: 0.5381\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6883 - acc: 0.5422\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6886 - acc: 0.5422\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6881 - acc: 0.5377\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6881 - acc: 0.5404\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6884 - acc: 0.5377\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6879 - acc: 0.5373\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6880 - acc: 0.5426\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6879 - acc: 0.5350\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6875 - acc: 0.5400\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6870 - acc: 0.5398\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6867 - acc: 0.5455\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6869 - acc: 0.5385\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6871 - acc: 0.5428\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6867 - acc: 0.5463\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6869 - acc: 0.5480\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6870 - acc: 0.5439\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6873 - acc: 0.5416\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6865 - acc: 0.5500\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6860 - acc: 0.5445\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6868 - acc: 0.5432\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6861 - acc: 0.5469\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6868 - acc: 0.5434\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6865 - acc: 0.5402\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6863 - acc: 0.5402\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6860 - acc: 0.5467\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6864 - acc: 0.5461\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6864 - acc: 0.5432\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6860 - acc: 0.5488\n",
      "training time : 16.169754\n",
      "40/40 [==============================] - 2s 3ms/step - loss: 0.6866 - acc: 0.5320\n",
      "test time : 2.469295\n",
      "total time: 18.639049\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines1[:3200]\n",
    "lines_ut = lines2[:3200]\n",
    "test_t = test1[:800]\n",
    "test_ut = test2[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines3[:3200]\n",
    "lines_ut = lines4[:3200]\n",
    "test_t = test3[:800]\n",
    "test_ut = test4[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines5[:3200]\n",
    "lines_ut = lines6[:3200]\n",
    "test_t = test5[:800]\n",
    "test_ut = test6[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines7[:3200]\n",
    "lines_ut = lines8[:3200]\n",
    "test_t = test7[:800]\n",
    "test_ut = test8[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines9[:3200]\n",
    "lines_ut = lines10[:3200]\n",
    "test_t = test9[:800]\n",
    "test_ut = test10[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "lines_t = lines11[:3200]\n",
    "lines_ut = lines12[:3200]\n",
    "test_t = test11[:800]\n",
    "test_ut = test12[:800]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "start1 = time.time()    \n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "input_train = input_train.reshape(-1,1,3)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# 'binary_crossentropy\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, mode='auto')\n",
    "\n",
    "history = model.fit(input_train, train_labels,\n",
    "                    epochs=1000,\n",
    "                    batch_size=128,callbacks=[early_stopping]\n",
    "                    )\n",
    "result1 = time.time() - start1\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)\n",
    "input_test = input_test.reshape(-1,1,3)\n",
    "\n",
    "model.evaluate(input_test, test_labels)\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63e9e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.514340333333335\n",
      "2.522034833333333\n",
      "17.036375166666666\n"
     ]
    }
   ],
   "source": [
    "print((11.749176+15.073475+16.789660+11.420784+15.883193+16.169754)/6)\n",
    "print((2.645432+2.406204+2.530867+ 2.515663+2.564748+2.469295)/6)\n",
    "print((14.394608+17.479679+19.320527+13.936447+18.447941+18.639049)/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b149b3",
   "metadata": {},
   "source": [
    "## svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d6876ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time : 4.180480\n",
      "0.56953125\n",
      "test time : 0.766193\n",
      "total time: 4.946673\n",
      "training time : 3.463343\n",
      "0.54609375\n",
      "test time : 1.402131\n",
      "total time: 4.865474\n",
      "training time : 3.806173\n",
      "0.5765625\n",
      "test time : 1.552353\n",
      "total time: 5.358526\n",
      "training time : 3.049978\n",
      "0.53828125\n",
      "test time : 1.559978\n",
      "total time: 4.609956\n",
      "training time : 3.450804\n",
      "0.55859375\n",
      "test time : 1.524499\n",
      "total time: 4.975303\n",
      "training time : 3.399824\n",
      "0.55234375\n",
      "test time : 1.407790\n",
      "total time: 4.807614\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "lines_t = lines1[:2560]\n",
    "lines_ut = lines2[:2560]\n",
    "test_t = test1[:640]\n",
    "test_ut = test2[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "\n",
    "####################################################\n",
    "\n",
    "lines_t = lines3[:2560]\n",
    "lines_ut = lines4[:2560]\n",
    "test_t = test3[:640]\n",
    "test_ut = test4[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "##############################\n",
    "lines_t = lines5[:2560]\n",
    "lines_ut = lines6[:2560]\n",
    "test_t = test5[:640]\n",
    "test_ut = test6[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "################################\n",
    "lines_t = lines7[:2560]\n",
    "lines_ut = lines8[:2560]\n",
    "test_t = test7[:640]\n",
    "test_ut = test8[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "########################################\n",
    "lines_t = lines9[:2560]\n",
    "lines_ut = lines10[:2560]\n",
    "test_t = test9[:640]\n",
    "test_ut = test10[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n",
    "#########################\n",
    "lines_t = lines11[:2560]\n",
    "lines_ut = lines12[:2560]\n",
    "test_t = test11[:640]\n",
    "test_ut = test12[:640]\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "test_lines_ = []\n",
    "\n",
    "lines1_ = []\n",
    "lines2_ = []\n",
    "lines3_ = []\n",
    "lines4_ = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "rating = []\n",
    "sentiment = []\n",
    "correlation = []\n",
    "\n",
    "for line in lines_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "    \n",
    "    \n",
    "\n",
    "for line in lines_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature1.append(a)\n",
    "\n",
    "for line in test_t:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "for line in test_ut:\n",
    "    a = line.split(\",\")[:3]\n",
    "    a = list(map(float, a))\n",
    "    feature2.append(a)\n",
    "    \n",
    "import time\n",
    "start1 = time.time()\n",
    "train_labels = [] # train 데이터 label\n",
    "test_labels = [] # test 데이터 label\n",
    "for i in range(len(lines_t)):\n",
    "    train_labels.append(1)\n",
    "for j in range(len(lines_ut)):\n",
    "    train_labels.append(0)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "input_train = np.asarray(feature1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C= 10, gamma=0.1)\n",
    "svc.fit(input_train, train_labels)\n",
    "result1 = time.time() - start1\n",
    "\n",
    "print(\"training time : %f\" %result1)\n",
    "start2 = time.time()\n",
    "for i in range(len(test_t)):\n",
    "    test_labels.append(1)\n",
    "for j in range(len(test_ut)):\n",
    "    test_labels.append(0)\n",
    "test_labels = np.asarray(test_labels)\n",
    "input_test = np.asarray(feature2)   \n",
    "print(svc.score(input_test, test_labels))\n",
    "result2 = time.time() - start2\n",
    "print(\"test time : %f\" %result2)\n",
    "print(\"total time: %f\" %(result1+result2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4698f218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5584336666666663\n",
      "1.368824\n",
      "4.927257666666667\n"
     ]
    }
   ],
   "source": [
    "print((4.180480+3.463343+3.806173+3.049978+3.450804+3.399824)/6)\n",
    "print((0.766193+1.402131+1.552353+1.559978+1.524499+1.407790)/6)\n",
    "print((4.946673+ 4.865474+5.358526+4.609956+4.975303+4.807614)/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da2cf07",
   "metadata": {},
   "source": [
    "# 시간측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ed07d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm\n",
      "14.042755666666666\n",
      "2.620745111111111\n",
      "16.663500666666668\n"
     ]
    }
   ],
   "source": [
    "print(\"lstm\")\n",
    "print((13.8502485+14.514340333333335+13.763678166666667)/3)\n",
    "print((2.7151236666666665+2.6250768333333334+2.522034833333333)/3)\n",
    "print((16.565372+16.388754833333333+17.036375166666666)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "038e04fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "3.5322535333333334\n",
      "1.3172658888888888\n",
      "4.849519388888889\n"
     ]
    }
   ],
   "source": [
    "print(\"svm\")\n",
    "print((3.5584336666666663+3.5495504333333336+3.4887764999999997)/3)\n",
    "print((1.368824+1.3897258333333333+1.1932478333333332)/3)\n",
    "print((4.927257666666667+4.939276166666667+4.682024333333334)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0c131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
